{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7Uhfwc1Nhc5+0ptNmJ9hy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anouardbt/dbt-cloud-api-demos/blob/main/dbt_Cloud%20Logs%20to%20GCS%20and%20BigQuery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c81Qs6E0DdQy"
      },
      "outputs": [],
      "source": [
        "!pip install google-auth-oauthlib google-cloud-logging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-storage"
      ],
      "metadata": {
        "id": "GXmhMMiUr6mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-storage google-cloud-bigquery"
      ],
      "metadata": {
        "id": "UaC6aU2e5mSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from google.cloud import logging\n",
        "#from google.cloud.logging_v2.resource import Resource\n",
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "import json\n",
        "from datetime import datetime\n",
        "import sys\n"
      ],
      "metadata": {
        "id": "CDKe09iKDkUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "creds, project = default()"
      ],
      "metadata": {
        "id": "uZpZz299Dl_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dbt Cloud API configuration\n",
        "DBT_CLOUD_ACCOUNT_ID = \"70403103916174\"  # Replace with your actual account ID\n",
        "DBT_CLOUD_API_KEY = \"Insert your Token Here\"  # Replace with your actual API key\n",
        "DBT_CLOUD_API_BASE_URL = 'https://c1.us1.dbt.com/api/v2'\n",
        "\n",
        "# GCP configuration\n",
        "GCP_PROJECT_ID = 'sales-demo-project-314714'  # Using the project from default credentials\n",
        "GCS_BUCKET_NAME = \"dbt-cloud-logging\"\n",
        "BIGQUERY_DATASET = \"ani_experiments\"  # Replace with your BigQuery dataset name\n",
        "BIGQUERY_TABLE_PREFIX = \"dbt_artifacts_\"  # Table prefix for partitioned BigQuery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPmIN_JTDpB9",
        "outputId": "37a13a1c-a4e2-433d-923a-d3bb4901c855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sales-demo-project-314714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Google Cloud Storage and BigQuery clients\n",
        "storage_client = storage.Client(project=GCP_PROJECT_ID, credentials=creds)\n",
        "bq_client = bigquery.Client(project=GCP_PROJECT_ID, credentials=creds)"
      ],
      "metadata": {
        "id": "eX0EgPFR6GRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dbt_cloud_artifacts(run_id):\n",
        "    url = f\"{DBT_CLOUD_API_BASE_URL}/accounts/{DBT_CLOUD_ACCOUNT_ID}/runs/{run_id}/artifacts/\"\n",
        "    headers = {\n",
        "        'Authorization': f'Token {DBT_CLOUD_API_KEY}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "9i7-4sQADrXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_artifact_content(run_id, path):\n",
        "    \"\"\"Fetches the content of a specific artifact and handles both JSON and non-JSON files.\"\"\"\n",
        "    url = f\"{DBT_CLOUD_API_BASE_URL}/accounts/{DBT_CLOUD_ACCOUNT_ID}/runs/{run_id}/artifacts/{path}\"\n",
        "    headers = {\n",
        "        'Authorization': f'Token {DBT_CLOUD_API_KEY}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Try to parse the content as JSON; if it fails, treat it as text\n",
        "    try:\n",
        "        return response.json()  # Return JSON if it is a valid JSON response\n",
        "    except ValueError:\n",
        "        return response.text  # If it's not JSON, return as text (e.g., SQL or other file types)"
      ],
      "metadata": {
        "id": "H4A-bTsODuVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_to_gcs(content, gcs_path):\n",
        "    \"\"\"Uploads the content to Google Cloud Storage (private by default).\"\"\"\n",
        "    # Reference the GCS bucket\n",
        "    bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "    blob = bucket.blob(gcs_path)\n",
        "\n",
        "    # Convert content to string and upload\n",
        "    blob.upload_from_string(json.dumps(content), content_type=\"application/json\")\n",
        "\n",
        "    # Keep the blob private (no blob.make_public())\n",
        "\n",
        "    print(f\"Artifact uploaded to GCS: {blob.name}\")\n",
        "    return f\"gs://{GCS_BUCKET_NAME}/{gcs_path}\"  # Return GCS URI for private blob"
      ],
      "metadata": {
        "id": "C0mXMmQ7sihr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_to_bigquery(content, artifact_path, run_id):\n",
        "    \"\"\"Uploads nested JSON content to BigQuery in a single column and partitions by system timestamp, including run_id.\"\"\"\n",
        "    # Sanitize the artifact_path to create a valid BigQuery table name and adding prefix\n",
        "    sanitized_table_name = artifact_path.replace('/', '_').replace('.', '_')\n",
        "\n",
        "    full_table_name = f\"{BIGQUERY_TABLE_PREFIX}{sanitized_table_name}\"\n",
        "\n",
        "    # Generate the current timestamp from the system\n",
        "    timestamp = datetime.utcnow().isoformat()  # Use the current timestamp for partitioning\n",
        "\n",
        "    # Define the table name\n",
        "    table_id = f\"{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{full_table_name}\"\n",
        "\n",
        "    # Define schema with three columns: JSON content, partitioning timestamp, and run_id\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"json_data\", \"JSON\"),  # Store entire JSON content in this field\n",
        "        bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),  # Store the current system timestamp for partitioning\n",
        "        bigquery.SchemaField(\"run_id\", \"INTEGER\")  # Add run_id as an integer column\n",
        "    ]\n",
        "\n",
        "    # Define table options with partitioning by system-generated timestamp\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,  # Append to the existing table\n",
        "        time_partitioning=bigquery.TimePartitioning(\n",
        "            type_=bigquery.TimePartitioningType.DAY,\n",
        "            field=\"timestamp\"  # Partition by system-generated timestamp\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Wrap content into a single field for BigQuery JSON column, and include the generated system timestamp and run_id\n",
        "    row = {\"json_data\": content, \"timestamp\": timestamp, \"run_id\": run_id}\n",
        "\n",
        "    # Upload the content as a single row to BigQuery\n",
        "    load_job = bq_client.load_table_from_json([row], table_id, job_config=job_config)\n",
        "    load_job.result()  # Wait for the job to complete\n",
        "\n",
        "    print(f\"Uploaded nested JSON artifact {artifact_path} with run_id {run_id} to BigQuery table {sanitized_table_name}\")"
      ],
      "metadata": {
        "id": "SuCc4DuV6Uqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_run_artifacts(run_id):\n",
        "    \"\"\"Fetches all artifacts from dbt cloud, uploads them to GCS, and uploads JSON files to BigQuery.\"\"\"\n",
        "    try:\n",
        "        # Fetch the list of artifacts\n",
        "        response = get_dbt_cloud_artifacts(run_id)\n",
        "\n",
        "        if not isinstance(response, dict):\n",
        "            print(f\"Unexpected response format. Expected a dict, got: {type(response)}\")\n",
        "            return\n",
        "\n",
        "        # Extract artifacts from the response (all artifacts are processed)\n",
        "        artifacts = response.get('data', response.get('results', None))\n",
        "\n",
        "        if not isinstance(artifacts, list):\n",
        "            print(f\"Artifacts are not in a list format. Got: {type(artifacts)}\")\n",
        "            return\n",
        "\n",
        "        for artifact in artifacts:\n",
        "            # Extract artifact path\n",
        "            if isinstance(artifact, str):\n",
        "                artifact_path = artifact\n",
        "            elif isinstance(artifact, dict):\n",
        "                artifact_path = artifact.get('path')\n",
        "                if not artifact_path:\n",
        "                    print(f\"No 'path' key in artifact: {artifact}\")\n",
        "                    continue\n",
        "            else:\n",
        "                print(f\"Unexpected artifact format. Expected a string or dict, got: {type(artifact)}\")\n",
        "                continue\n",
        "\n",
        "            # Fetch the artifact content (handles both JSON and non-JSON files)\n",
        "            try:\n",
        "                artifact_content = get_artifact_content(run_id, artifact_path)\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error fetching artifact content for path {artifact_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Upload artifact content to GCS (private by default)\n",
        "            gcs_path = f\"dbt_artifacts/{run_id}/{artifact_path.replace('/', '_')}\"\n",
        "            gcs_uri = upload_to_gcs(artifact_content, gcs_path)\n",
        "\n",
        "            # If the content is JSON, upload it to BigQuery\n",
        "            if isinstance(artifact_content, dict):\n",
        "                upload_to_bigquery(artifact_content, artifact_path, run_id)\n",
        "            else:\n",
        "                print(f\"Non-JSON artifact {artifact_path} saved to GCS but not uploaded to BigQuery.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching artifacts: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "        print(f\"Error details: {sys.exc_info()}\")\n"
      ],
      "metadata": {
        "id": "0uBF-xpwD4_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Example usage\n",
        "    run_id = 70403121082874  # Replace with actual run ID\n",
        "    process_run_artifacts(run_id)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "kljIWl1PD7yW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}